{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# 連接到資料庫A ====================================================================\n",
    "conn = sqlite3.connect('goodinfoRevenue.db')\n",
    "# cursor object\n",
    "cursor = conn.cursor()\n",
    "\n",
    "sql = '''select * from revenue t'''\n",
    "\n",
    "# 連接到資料庫B\n",
    "sqlStock = '''select * from stock t'''\n",
    "dfStock = pd.read_sql(sqlStock,conn)\n",
    "dfStock = pd.DataFrame(dfStock)\n",
    "ids = dfStock['code']\n",
    "# ===================================================================================\n",
    "\n",
    "count=0\n",
    "\n",
    "for id in ids:\n",
    "    print(id)\n",
    "\n",
    "    # 上個月財報日期\n",
    "    thisMonth = pd.Timestamp.today() \n",
    "    LastMonth = thisMonth - pd.DateOffset(months=1) # 這個月日期減上個月日期\n",
    "    LastMonth = LastMonth.strftime(\"%Y/%m\") # 格式化日期\n",
    "\n",
    "    # 先查詢本地資料庫\n",
    "    dfdatabase = pd.read_sql(sql,conn)\n",
    "    dfdatabaseMask = dfdatabase['code'] == id\n",
    "    dfdatabase2 = dfdatabase[dfdatabaseMask]\n",
    "    dfdatabase2\n",
    "\n",
    "    # 如果沒有上個月資料則爬蟲更新\n",
    "    if LastMonth not in dfdatabase2['date'].values:\n",
    "        \n",
    "        # 爬個股資料\n",
    "        url = f'https://goodinfo.tw/tw/ShowSaleMonChart.asp?STOCK_ID={id}'\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36 Edg/104.0.1293.70'} # goodinfo有擋機器人爬蟲，透過添加headers模仿真實上網的環境就能抓到資料了\n",
    "        res = requests.get(url, headers = headers,timeout=20)\n",
    "        res.encoding = \"utf-8\" # 將編碼設定為【utf-8】，中文字就能顯示出來了\n",
    "        # res.text\n",
    "\n",
    "        # BeautifSoup是一個用來解析HTML結構的Python套件，將取回的網頁HTML結構透過提供的方法解析。解析器（html.parser,html5lib,lxml），官方文件lxml為最快\n",
    "        soup = BeautifulSoup(res.text,\"lxml\") \n",
    "        # select_one：搜索類名、標籤名、id名等，因為我們搜索的是id，在html語言中要加【#】才能搜索到\n",
    "        data = soup.select_one(\"#divSaleMonChartDetail\")\n",
    "        # data\n",
    "\n",
    "\n",
    "        # 【重整表格】 ===============================================================================================\n",
    "        # prettify()：函數將我們的data物件美化作用\n",
    "        dfs = pd.read_html(data.prettify())\n",
    "        df = dfs[1]\n",
    "        # 網頁的表格是由四格組成，但Python中無法合併單元格一起顯示，所以被合併的表格就會拆分成一格一格顯示\n",
    "        # 使用columns.get_level_values來取得的最後一行的欄位名\n",
    "        df.columns = df.columns.get_level_values(2)\n",
    "        # 刪除所有多於的標題欄\n",
    "        df2 = df[df[\"月別\"]==\"月別\"].index\n",
    "        df2 = df.drop(df2)\n",
    "        # 重整標題\n",
    "        # df2.columns = ['月別','開盤','收盤','最高','最低','漲跌(元)','漲跌(%)','月營收(億)','月月增(%)','月年增','累月營收(億)','累月年增','營收(億)','月增(%)','年增(%)','累計營收(億)','累計年增(%)']\n",
    "        df2.columns = ['date','open','close','high','low','updownYuan','updown','月營收(億)','月月增(%)','月年增','累月營收(億)','累月年增','revenue','mon','yoy','revenueSum','yoySum']\n",
    "\n",
    "        # 刪除營業收入\n",
    "        df3 = df2.copy()\n",
    "        df3.drop(columns=['月營收(億)','月月增(%)','月年增','累月營收(億)','累月年增'],inplace=True)\n",
    "        # 使用pandas的insert方法，第一个参数指定插入列的位置，第二个参数指定插入列的列名，第三个参数指定插入列的数据\n",
    "        df3.insert(0,'code',id)\n",
    "        # =============================================================================================================\n",
    "\n",
    "\n",
    "        # 如果爬蟲沒有新資料則跳過\n",
    "        if LastMonth not in df3['date'].values:\n",
    "            pass\n",
    "        # 如果爬蟲有新資料則更新\n",
    "        else:\n",
    "            df3mask = df3['date'] == LastMonth\n",
    "            df4 = df3[df3mask]\n",
    "            for index, row in df4.iterrows():\n",
    "                try:\n",
    "                    cursor.execute(\n",
    "                    \"\"\"INSERT OR IGNORE INTO revenue \n",
    "                        (code,date,open,close,high,low,updownYuan,updown,revenue,mon,yoy,revenueSum,yoySum)\n",
    "                        values(?,?,?,?,?,?,?,?,?,?,?,?,?)\"\"\",\n",
    "                        (row['code'],\n",
    "                        row['date'],\n",
    "                        row['open'],\n",
    "                        row['close'],\n",
    "                        row['high'],\n",
    "                        row['low'],\n",
    "                        row['updownYuan'],\n",
    "                        row['updown'],\n",
    "                        row['revenue'],\n",
    "                        row['mon'],\n",
    "                        row['yoy'],\n",
    "                        row['revenueSum'],\n",
    "                        row['yoySum'])\n",
    "                        )\n",
    "                    conn.commit()\n",
    "                except:\n",
    "                    pass\n",
    "    # 如果有的話則跳過\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    count+=1\n",
    "    print(f'已完成{count}次')\n",
    "    time.sleep(75)\n",
    "    \n",
    "# =============================================================================================================\n",
    "# pd.read_sql(sql,conn)\n",
    "print('爬蟲完畢')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d17bd3c188470cc3cbfd93c90c2f8b3f5ab1a762ae997509e3112b6584543eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
